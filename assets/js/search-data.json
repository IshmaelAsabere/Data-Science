{
  
    
        "post0": {
            "title": "Introduction to Statistics for Data Analysis with Python",
            "content": "Learning objective . What shall be your key takeaways from this project? . What are some good questions to ask looking at a dataset? | Data Wrangling / Data Cleansing | Exploratory Data Analysis | Collaborate, visualise, and communicate. | Background . What can we say about the success of a movie before it is released? Are there certain companies (Pixar?) that have found a consistent formula? Given that major films costing over $100 million to produce can still flop, this question is more important than ever to the industry. Film aficionados might have different interests. Can we predict which films will be highly rated, whether or not they are a commercial success? . This is a great place to start digging in to those questions, with data on the plot, cast, crew, budget, and revenues of several thousand films. . # Importing the required packages here import numpy as np import pandas as pd import seaborn as sns import ast, json from datetime import datetime import matplotlib.pyplot as plt import warnings warnings.filterwarnings(&#39;ignore&#39;) . Let&#39;s load the data . # Let&#39;s load the dataset and create their dataframes credits_df = pd.read_csv(&#39;../data/tmdb_5000_credits.csv&#39;) movies_df = pd.read_csv(&#39;../data/tmdb_5000_movies.csv&#39;) movies_df.head() . budget genres homepage id keywords original_language original_title overview popularity production_companies production_countries release_date revenue runtime spoken_languages status tagline title vote_average vote_count . 0 | 237000000 | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | http://www.avatarmovie.com/ | 19995 | [{&quot;id&quot;: 1463, &quot;name&quot;: &quot;culture clash&quot;}, {&quot;id&quot;:... | en | Avatar | In the 22nd century, a paraplegic Marine is di... | 150.437577 | [{&quot;name&quot;: &quot;Ingenious Film Partners&quot;, &quot;id&quot;: 289... | [{&quot;iso_3166_1&quot;: &quot;US&quot;, &quot;name&quot;: &quot;United States o... | 2009-12-10 | 2787965087 | 162.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}, {&quot;iso... | Released | Enter the World of Pandora. | Avatar | 7.2 | 11800 | . 1 | 300000000 | [{&quot;id&quot;: 12, &quot;name&quot;: &quot;Adventure&quot;}, {&quot;id&quot;: 14, &quot;... | http://disney.go.com/disneypictures/pirates/ | 285 | [{&quot;id&quot;: 270, &quot;name&quot;: &quot;ocean&quot;}, {&quot;id&quot;: 726, &quot;na... | en | Pirates of the Caribbean: At World&#39;s End | Captain Barbossa, long believed to be dead, ha... | 139.082615 | [{&quot;name&quot;: &quot;Walt Disney Pictures&quot;, &quot;id&quot;: 2}, {&quot;... | [{&quot;iso_3166_1&quot;: &quot;US&quot;, &quot;name&quot;: &quot;United States o... | 2007-05-19 | 961000000 | 169.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}] | Released | At the end of the world, the adventure begins. | Pirates of the Caribbean: At World&#39;s End | 6.9 | 4500 | . 2 | 245000000 | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | http://www.sonypictures.com/movies/spectre/ | 206647 | [{&quot;id&quot;: 470, &quot;name&quot;: &quot;spy&quot;}, {&quot;id&quot;: 818, &quot;name... | en | Spectre | A cryptic message from Bond’s past sends him o... | 107.376788 | [{&quot;name&quot;: &quot;Columbia Pictures&quot;, &quot;id&quot;: 5}, {&quot;nam... | [{&quot;iso_3166_1&quot;: &quot;GB&quot;, &quot;name&quot;: &quot;United Kingdom&quot;... | 2015-10-26 | 880674609 | 148.0 | [{&quot;iso_639_1&quot;: &quot;fr&quot;, &quot;name&quot;: &quot;Fran u00e7ais&quot;},... | Released | A Plan No One Escapes | Spectre | 6.3 | 4466 | . 3 | 250000000 | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 80, &quot;nam... | http://www.thedarkknightrises.com/ | 49026 | [{&quot;id&quot;: 849, &quot;name&quot;: &quot;dc comics&quot;}, {&quot;id&quot;: 853,... | en | The Dark Knight Rises | Following the death of District Attorney Harve... | 112.312950 | [{&quot;name&quot;: &quot;Legendary Pictures&quot;, &quot;id&quot;: 923}, {&quot;... | [{&quot;iso_3166_1&quot;: &quot;US&quot;, &quot;name&quot;: &quot;United States o... | 2012-07-16 | 1084939099 | 165.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}] | Released | The Legend Ends | The Dark Knight Rises | 7.6 | 9106 | . 4 | 260000000 | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | http://movies.disney.com/john-carter | 49529 | [{&quot;id&quot;: 818, &quot;name&quot;: &quot;based on novel&quot;}, {&quot;id&quot;:... | en | John Carter | John Carter is a war-weary, former military ca... | 43.926995 | [{&quot;name&quot;: &quot;Walt Disney Pictures&quot;, &quot;id&quot;: 2}] | [{&quot;iso_3166_1&quot;: &quot;US&quot;, &quot;name&quot;: &quot;United States o... | 2012-03-07 | 284139100 | 132.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}] | Released | Lost in our world, found in another. | John Carter | 6.1 | 2124 | . Looking at this dataset, we need to define a set of questions. . Let&#39;s define those primary questions here: . Which are the 5 most expensive movies? Comparison between the extremes. | Top 5 most profitable movies? Comparison between the min and max profits. | Most talked about Movies? | Average runtime of movies? | Movies which are rated above 7 by the critics? | Which year did we have the most profitable movies? | Then there will be a set of seconday questions related to the questions above . Most successful genre. | Frequent Cast in movies. | Average budget of the profitable movies. | Average duration of the most profitable movies. | Language of the most profitable movies. | # merging the two files movies_df = pd.merge(movies_df, credits_df, left_on =&#39;id&#39;, right_on=&#39;movie_id&#39;, how=&#39;left&#39;, suffixes=(&#39;&#39;, &#39;_y&#39;)) movies_df.columns . Index([&#39;budget&#39;, &#39;genres&#39;, &#39;homepage&#39;, &#39;id&#39;, &#39;keywords&#39;, &#39;original_language&#39;, &#39;original_title&#39;, &#39;overview&#39;, &#39;popularity&#39;, &#39;production_companies&#39;, &#39;production_countries&#39;, &#39;release_date&#39;, &#39;revenue&#39;, &#39;runtime&#39;, &#39;spoken_languages&#39;, &#39;status&#39;, &#39;tagline&#39;, &#39;title&#39;, &#39;vote_average&#39;, &#39;vote_count&#39;, &#39;movie_id&#39;, &#39;title_y&#39;, &#39;cast&#39;, &#39;crew&#39;], dtype=&#39;object&#39;) . Data Cleaning Process . After observing the dataset and proposed questions for the analysis we will be keeping only relevent data deleting the unsued data so that we can make our calculation easy and understandable. . . Steps to be taken to clean the data. . We need to remove unused column such as id, imdb_id, vote_count, production_company, keywords, homepage etc. | Removing the duplicacy in the rows(if any). | Some movies in the database have zero budget or zero revenue, that is there value has not been recorded so we will be discarding such entries. | Changing release date column into date format. | Replacing zero with NAN in runtime column. | Changing format of budget and revenue column. | # First step is to clean the data and see which are the redundant or unnecessary cols del_col_list = [&#39;keywords&#39;, &#39;homepage&#39;, &#39;status&#39;, &#39;tagline&#39;, &#39;original_language&#39;, &#39;homepage&#39;, &#39;overview&#39;, &#39;production_companies&#39;, &#39;original_title&#39;, &#39;title_y&#39;] movies_df = movies_df.drop(del_col_list, axis=1) movies_df.head() . budget genres id popularity production_countries release_date revenue runtime spoken_languages title vote_average vote_count movie_id cast crew . 0 | 237000000 | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | 19995 | 150.437577 | [{&quot;iso_3166_1&quot;: &quot;US&quot;, &quot;name&quot;: &quot;United States o... | 2009-12-10 | 2787965087 | 162.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}, {&quot;iso... | Avatar | 7.2 | 11800 | 19995 | [{&quot;cast_id&quot;: 242, &quot;character&quot;: &quot;Jake Sully&quot;, &quot;... | [{&quot;credit_id&quot;: &quot;52fe48009251416c750aca23&quot;, &quot;de... | . 1 | 300000000 | [{&quot;id&quot;: 12, &quot;name&quot;: &quot;Adventure&quot;}, {&quot;id&quot;: 14, &quot;... | 285 | 139.082615 | [{&quot;iso_3166_1&quot;: &quot;US&quot;, &quot;name&quot;: &quot;United States o... | 2007-05-19 | 961000000 | 169.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}] | Pirates of the Caribbean: At World&#39;s End | 6.9 | 4500 | 285 | [{&quot;cast_id&quot;: 4, &quot;character&quot;: &quot;Captain Jack Spa... | [{&quot;credit_id&quot;: &quot;52fe4232c3a36847f800b579&quot;, &quot;de... | . 2 | 245000000 | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | 206647 | 107.376788 | [{&quot;iso_3166_1&quot;: &quot;GB&quot;, &quot;name&quot;: &quot;United Kingdom&quot;... | 2015-10-26 | 880674609 | 148.0 | [{&quot;iso_639_1&quot;: &quot;fr&quot;, &quot;name&quot;: &quot;Fran u00e7ais&quot;},... | Spectre | 6.3 | 4466 | 206647 | [{&quot;cast_id&quot;: 1, &quot;character&quot;: &quot;James Bond&quot;, &quot;cr... | [{&quot;credit_id&quot;: &quot;54805967c3a36829b5002c41&quot;, &quot;de... | . 3 | 250000000 | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 80, &quot;nam... | 49026 | 112.312950 | [{&quot;iso_3166_1&quot;: &quot;US&quot;, &quot;name&quot;: &quot;United States o... | 2012-07-16 | 1084939099 | 165.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}] | The Dark Knight Rises | 7.6 | 9106 | 49026 | [{&quot;cast_id&quot;: 2, &quot;character&quot;: &quot;Bruce Wayne / Ba... | [{&quot;credit_id&quot;: &quot;52fe4781c3a36847f81398c3&quot;, &quot;de... | . 4 | 260000000 | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | 49529 | 43.926995 | [{&quot;iso_3166_1&quot;: &quot;US&quot;, &quot;name&quot;: &quot;United States o... | 2012-03-07 | 284139100 | 132.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}] | John Carter | 6.1 | 2124 | 49529 | [{&quot;cast_id&quot;: 5, &quot;character&quot;: &quot;John Carter&quot;, &quot;c... | [{&quot;credit_id&quot;: &quot;52fe479ac3a36847f813eaa3&quot;, &quot;de... | . How to handle the Json in Dataset? . The main problem with this dataset is the .json format. Many columns in the dataset are in json format, therefore cleaning the dataset was the main challenge. For people who don&#39;t know about JSON(JavaScript Object Notation), it is basically a syntax for storing and exchanging data between two computers. It is mainly in a key:value format, and is embedded into a string. . # we see that there are columns which are in json format, # let&#39;s flatten these json data into easyily interpretable lists def parse_col_json(column, key): &quot;&quot;&quot; Args: column: string name of the column to be processed. key: string name of the dictionary key which needs to be extracted &quot;&quot;&quot; for index,i in zip(movies_df.index,movies_df[column].apply(json.loads)): list1=[] for j in range(len(i)): list1.append((i[j][key]))# the key &#39;name&#39; contains the name of the genre movies_df.loc[index,column]=str(list1) parse_col_json(&#39;genres&#39;, &#39;name&#39;) parse_col_json(&#39;spoken_languages&#39;, &#39;name&#39;) parse_col_json(&#39;cast&#39;, &#39;name&#39;) parse_col_json(&#39;production_countries&#39;, &#39;name&#39;) movies_df.head() . budget genres id popularity production_countries release_date revenue runtime spoken_languages title vote_average vote_count movie_id cast crew . 0 | 237000000 | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Fantasy&#39;, &#39;Science Fi... | 19995 | 150.437577 | [&#39;United States of America&#39;, &#39;United Kingdom&#39;] | 2009-12-10 | 2787965087 | 162.0 | [&#39;English&#39;, &#39;Español&#39;] | Avatar | 7.2 | 11800 | 19995 | [&#39;Sam Worthington&#39;, &#39;Zoe Saldana&#39;, &#39;Sigourney ... | [{&quot;credit_id&quot;: &quot;52fe48009251416c750aca23&quot;, &quot;de... | . 1 | 300000000 | [&#39;Adventure&#39;, &#39;Fantasy&#39;, &#39;Action&#39;] | 285 | 139.082615 | [&#39;United States of America&#39;] | 2007-05-19 | 961000000 | 169.0 | [&#39;English&#39;] | Pirates of the Caribbean: At World&#39;s End | 6.9 | 4500 | 285 | [&#39;Johnny Depp&#39;, &#39;Orlando Bloom&#39;, &#39;Keira Knight... | [{&quot;credit_id&quot;: &quot;52fe4232c3a36847f800b579&quot;, &quot;de... | . 2 | 245000000 | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Crime&#39;] | 206647 | 107.376788 | [&#39;United Kingdom&#39;, &#39;United States of America&#39;] | 2015-10-26 | 880674609 | 148.0 | [&#39;Français&#39;, &#39;English&#39;, &#39;Español&#39;, &#39;Italiano&#39;,... | Spectre | 6.3 | 4466 | 206647 | [&#39;Daniel Craig&#39;, &#39;Christoph Waltz&#39;, &#39;Léa Seydo... | [{&quot;credit_id&quot;: &quot;54805967c3a36829b5002c41&quot;, &quot;de... | . 3 | 250000000 | [&#39;Action&#39;, &#39;Crime&#39;, &#39;Drama&#39;, &#39;Thriller&#39;] | 49026 | 112.312950 | [&#39;United States of America&#39;] | 2012-07-16 | 1084939099 | 165.0 | [&#39;English&#39;] | The Dark Knight Rises | 7.6 | 9106 | 49026 | [&#39;Christian Bale&#39;, &#39;Michael Caine&#39;, &#39;Gary Oldm... | [{&quot;credit_id&quot;: &quot;52fe4781c3a36847f81398c3&quot;, &quot;de... | . 4 | 260000000 | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Science Fiction&#39;] | 49529 | 43.926995 | [&#39;United States of America&#39;] | 2012-03-07 | 284139100 | 132.0 | [&#39;English&#39;] | John Carter | 6.1 | 2124 | 49529 | [&#39;Taylor Kitsch&#39;, &#39;Lynn Collins&#39;, &#39;Samantha Mo... | [{&quot;credit_id&quot;: &quot;52fe479ac3a36847f813eaa3&quot;, &quot;de... | . # dropping the duplicates from the dataset. print(movies_df.shape) movies_df = movies_df.drop_duplicates(keep=&#39;first&#39;) print(movies_df.shape) . (4803, 15) (4803, 15) . # replacing all the zeros from revenue and budget cols. cols = [&#39;budget&#39;, &#39;revenue&#39;] movies_df[cols] = movies_df[cols].replace(0, np.nan) # dropping all the rows with na in the columns mentioned above in the list. movies_df.dropna(subset=cols, inplace=True) movies_df.shape . (3229, 15) . # Changing the release_date column to DateTime column movies_df.release_date = pd.to_datetime(movies_df[&#39;release_date&#39;]) movies_df.head() . budget genres id popularity production_countries release_date revenue runtime spoken_languages title vote_average vote_count movie_id cast crew . 0 | 237000000.0 | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Fantasy&#39;, &#39;Science Fi... | 19995 | 150.437577 | [&#39;United States of America&#39;, &#39;United Kingdom&#39;] | 2009-12-10 | 2.787965e+09 | 162.0 | [&#39;English&#39;, &#39;Español&#39;] | Avatar | 7.2 | 11800 | 19995 | [&#39;Sam Worthington&#39;, &#39;Zoe Saldana&#39;, &#39;Sigourney ... | [{&quot;credit_id&quot;: &quot;52fe48009251416c750aca23&quot;, &quot;de... | . 1 | 300000000.0 | [&#39;Adventure&#39;, &#39;Fantasy&#39;, &#39;Action&#39;] | 285 | 139.082615 | [&#39;United States of America&#39;] | 2007-05-19 | 9.610000e+08 | 169.0 | [&#39;English&#39;] | Pirates of the Caribbean: At World&#39;s End | 6.9 | 4500 | 285 | [&#39;Johnny Depp&#39;, &#39;Orlando Bloom&#39;, &#39;Keira Knight... | [{&quot;credit_id&quot;: &quot;52fe4232c3a36847f800b579&quot;, &quot;de... | . 2 | 245000000.0 | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Crime&#39;] | 206647 | 107.376788 | [&#39;United Kingdom&#39;, &#39;United States of America&#39;] | 2015-10-26 | 8.806746e+08 | 148.0 | [&#39;Français&#39;, &#39;English&#39;, &#39;Español&#39;, &#39;Italiano&#39;,... | Spectre | 6.3 | 4466 | 206647 | [&#39;Daniel Craig&#39;, &#39;Christoph Waltz&#39;, &#39;Léa Seydo... | [{&quot;credit_id&quot;: &quot;54805967c3a36829b5002c41&quot;, &quot;de... | . 3 | 250000000.0 | [&#39;Action&#39;, &#39;Crime&#39;, &#39;Drama&#39;, &#39;Thriller&#39;] | 49026 | 112.312950 | [&#39;United States of America&#39;] | 2012-07-16 | 1.084939e+09 | 165.0 | [&#39;English&#39;] | The Dark Knight Rises | 7.6 | 9106 | 49026 | [&#39;Christian Bale&#39;, &#39;Michael Caine&#39;, &#39;Gary Oldm... | [{&quot;credit_id&quot;: &quot;52fe4781c3a36847f81398c3&quot;, &quot;de... | . 4 | 260000000.0 | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Science Fiction&#39;] | 49529 | 43.926995 | [&#39;United States of America&#39;] | 2012-03-07 | 2.841391e+08 | 132.0 | [&#39;English&#39;] | John Carter | 6.1 | 2124 | 49529 | [&#39;Taylor Kitsch&#39;, &#39;Lynn Collins&#39;, &#39;Samantha Mo... | [{&quot;credit_id&quot;: &quot;52fe479ac3a36847f813eaa3&quot;, &quot;de... | . To answer the last question, we&#39;ll be needing the release year from the release_date . # To answer the last question, we&#39;ll have to extract the release year from every release date movies_df[&#39;release_year&#39;] = movies_df[&#39;release_date&#39;].dt.year movies_df.head() . budget genres id popularity production_countries release_date revenue runtime spoken_languages title vote_average vote_count movie_id cast crew release_year . 0 | 237000000.0 | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Fantasy&#39;, &#39;Science Fi... | 19995 | 150.437577 | [&#39;United States of America&#39;, &#39;United Kingdom&#39;] | 2009-12-10 | 2.787965e+09 | 162.0 | [&#39;English&#39;, &#39;Español&#39;] | Avatar | 7.2 | 11800 | 19995 | [&#39;Sam Worthington&#39;, &#39;Zoe Saldana&#39;, &#39;Sigourney ... | [{&quot;credit_id&quot;: &quot;52fe48009251416c750aca23&quot;, &quot;de... | 2009 | . 1 | 300000000.0 | [&#39;Adventure&#39;, &#39;Fantasy&#39;, &#39;Action&#39;] | 285 | 139.082615 | [&#39;United States of America&#39;] | 2007-05-19 | 9.610000e+08 | 169.0 | [&#39;English&#39;] | Pirates of the Caribbean: At World&#39;s End | 6.9 | 4500 | 285 | [&#39;Johnny Depp&#39;, &#39;Orlando Bloom&#39;, &#39;Keira Knight... | [{&quot;credit_id&quot;: &quot;52fe4232c3a36847f800b579&quot;, &quot;de... | 2007 | . 2 | 245000000.0 | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Crime&#39;] | 206647 | 107.376788 | [&#39;United Kingdom&#39;, &#39;United States of America&#39;] | 2015-10-26 | 8.806746e+08 | 148.0 | [&#39;Français&#39;, &#39;English&#39;, &#39;Español&#39;, &#39;Italiano&#39;,... | Spectre | 6.3 | 4466 | 206647 | [&#39;Daniel Craig&#39;, &#39;Christoph Waltz&#39;, &#39;Léa Seydo... | [{&quot;credit_id&quot;: &quot;54805967c3a36829b5002c41&quot;, &quot;de... | 2015 | . 3 | 250000000.0 | [&#39;Action&#39;, &#39;Crime&#39;, &#39;Drama&#39;, &#39;Thriller&#39;] | 49026 | 112.312950 | [&#39;United States of America&#39;] | 2012-07-16 | 1.084939e+09 | 165.0 | [&#39;English&#39;] | The Dark Knight Rises | 7.6 | 9106 | 49026 | [&#39;Christian Bale&#39;, &#39;Michael Caine&#39;, &#39;Gary Oldm... | [{&quot;credit_id&quot;: &quot;52fe4781c3a36847f81398c3&quot;, &quot;de... | 2012 | . 4 | 260000000.0 | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Science Fiction&#39;] | 49529 | 43.926995 | [&#39;United States of America&#39;] | 2012-03-07 | 2.841391e+08 | 132.0 | [&#39;English&#39;] | John Carter | 6.1 | 2124 | 49529 | [&#39;Taylor Kitsch&#39;, &#39;Lynn Collins&#39;, &#39;Samantha Mo... | [{&quot;credit_id&quot;: &quot;52fe479ac3a36847f813eaa3&quot;, &quot;de... | 2012 | . # Changing the data type of the below mentioned columns and change_cols=[&#39;budget&#39;, &#39;revenue&#39;] #changing data type movies_df[change_cols]=movies_df[change_cols].applymap(np.int64) movies_df.dtypes . budget int64 genres object id int64 popularity float64 production_countries object release_date datetime64[ns] revenue int64 runtime float64 spoken_languages object title object vote_average float64 vote_count int64 movie_id int64 cast object crew object release_year int64 dtype: object . Let&#39;s answer Question #1 . # Answer to question #1. # To find out the most expensive movies, we need to look at the budget set for them which is an indicator of expense. expensive_movies_df = movies_df.sort_values(by =&#39;budget&#39;, ascending=False).head() expensive_movies_df # below are the 5 most expensive movies in descending order. . budget genres id popularity production_countries release_date revenue runtime spoken_languages title vote_average vote_count movie_id cast crew release_year . 17 | 380000000 | [&#39;Adventure&#39;, &#39;Action&#39;, &#39;Fantasy&#39;] | 1865 | 135.413856 | [&#39;United States of America&#39;] | 2011-05-14 | 1045713802 | 136.0 | [&#39;English&#39;, &#39;Español&#39;] | Pirates of the Caribbean: On Stranger Tides | 6.4 | 4948 | 1865 | [&#39;Johnny Depp&#39;, &#39;Penélope Cruz&#39;, &#39;Ian McShane&#39;... | [{&quot;credit_id&quot;: &quot;566b4f54c3a3683f56005151&quot;, &quot;de... | 2011 | . 1 | 300000000 | [&#39;Adventure&#39;, &#39;Fantasy&#39;, &#39;Action&#39;] | 285 | 139.082615 | [&#39;United States of America&#39;] | 2007-05-19 | 961000000 | 169.0 | [&#39;English&#39;] | Pirates of the Caribbean: At World&#39;s End | 6.9 | 4500 | 285 | [&#39;Johnny Depp&#39;, &#39;Orlando Bloom&#39;, &#39;Keira Knight... | [{&quot;credit_id&quot;: &quot;52fe4232c3a36847f800b579&quot;, &quot;de... | 2007 | . 7 | 280000000 | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Science Fiction&#39;] | 99861 | 134.279229 | [&#39;United States of America&#39;] | 2015-04-22 | 1405403694 | 141.0 | [&#39;English&#39;] | Avengers: Age of Ultron | 7.3 | 6767 | 99861 | [&#39;Robert Downey Jr.&#39;, &#39;Chris Hemsworth&#39;, &#39;Mark... | [{&quot;credit_id&quot;: &quot;55d5f7d4c3a3683e7e0016eb&quot;, &quot;de... | 2015 | . 10 | 270000000 | [&#39;Adventure&#39;, &#39;Fantasy&#39;, &#39;Action&#39;, &#39;Science Fi... | 1452 | 57.925623 | [&#39;United States of America&#39;] | 2006-06-28 | 391081192 | 154.0 | [&#39;English&#39;, &#39;Français&#39;, &#39;Deutsch&#39;] | Superman Returns | 5.4 | 1400 | 1452 | [&#39;Brandon Routh&#39;, &#39;Kevin Spacey&#39;, &#39;Kate Boswor... | [{&quot;credit_id&quot;: &quot;553bef6a9251416874003c8f&quot;, &quot;de... | 2006 | . 4 | 260000000 | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Science Fiction&#39;] | 49529 | 43.926995 | [&#39;United States of America&#39;] | 2012-03-07 | 284139100 | 132.0 | [&#39;English&#39;] | John Carter | 6.1 | 2124 | 49529 | [&#39;Taylor Kitsch&#39;, &#39;Lynn Collins&#39;, &#39;Samantha Mo... | [{&quot;credit_id&quot;: &quot;52fe479ac3a36847f813eaa3&quot;, &quot;de... | 2012 | . Since we need to compare the minimums and maximums in 3 questions, we can write a generi function to do that for us. It will remove all the redundancy in code for such questions. . def find_min_max_in(col): &quot;&quot;&quot; The function takes in a column and returns the top 5 and bottom 5 movies dataframe in that column. args: col: string - column name return: info_df: dataframe - final 5 movies dataframe &quot;&quot;&quot; top = movies_df[col].idxmax() top_df = pd.DataFrame(movies_df.loc[top]) bottom = movies_df[col].idxmin() bottom_df = pd.DataFrame(movies_df.loc[bottom]) info_df = pd.concat([top_df, bottom_df], axis=1) return info_df find_min_max_in(&#39;budget&#39;) . 17 4238 . budget | 380000000 | 1 | . genres | [&#39;Adventure&#39;, &#39;Action&#39;, &#39;Fantasy&#39;] | [&#39;Drama&#39;, &#39;Comedy&#39;] | . id | 1865 | 3082 | . popularity | 135.414 | 28.2765 | . production_countries | [&#39;United States of America&#39;] | [&#39;United States of America&#39;] | . release_date | 2011-05-14 00:00:00 | 1936-02-05 00:00:00 | . revenue | 1045713802 | 8500000 | . runtime | 136 | 87 | . spoken_languages | [&#39;English&#39;, &#39;Español&#39;] | [&#39;English&#39;] | . title | Pirates of the Caribbean: On Stranger Tides | Modern Times | . vote_average | 6.4 | 8.1 | . vote_count | 4948 | 856 | . movie_id | 1865 | 3082 | . cast | [&#39;Johnny Depp&#39;, &#39;Penélope Cruz&#39;, &#39;Ian McShane&#39;... | [&#39;Charlie Chaplin&#39;, &#39;Paulette Goddard&#39;, &#39;Henry... | . crew | [{&quot;credit_id&quot;: &quot;566b4f54c3a3683f56005151&quot;, &quot;de... | [{&quot;credit_id&quot;: &quot;5621aeadc3a3680e1d00a09a&quot;, &quot;de... | . release_year | 2011 | 1936 | . Answer to Question #2 . # to find the most profitable movies, we need to find who made the most # amount after deducting the budget from the revenue generated. movies_df[&#39;profit&#39;] = movies_df[&#39;revenue&#39;] - movies_df[&#39;budget&#39;] cols = [&#39;budget&#39;, &#39;profit&#39;, &#39;revenue&#39;, &#39;genres&#39;, &#39;id&#39;, &#39;popularity&#39;, &#39;production_countries&#39;, &#39;release_date&#39;, &#39;release_year&#39;, &#39;runtime&#39;, &#39;spoken_languages&#39;, &#39;title&#39;, &#39;cast&#39;, &#39;vote_average&#39;, &#39;vote_count&#39;] movies_df = movies_df[cols] movies_df.sort_values(by = [&#39;budget&#39;], ascending=False).head() . budget profit revenue genres id popularity production_countries release_date release_year runtime spoken_languages title cast vote_average vote_count . 17 | 380000000 | 665713802 | 1045713802 | [&#39;Adventure&#39;, &#39;Action&#39;, &#39;Fantasy&#39;] | 1865 | 135.413856 | [&#39;United States of America&#39;] | 2011-05-14 | 2011 | 136.0 | [&#39;English&#39;, &#39;Español&#39;] | Pirates of the Caribbean: On Stranger Tides | [&#39;Johnny Depp&#39;, &#39;Penélope Cruz&#39;, &#39;Ian McShane&#39;... | 6.4 | 4948 | . 1 | 300000000 | 661000000 | 961000000 | [&#39;Adventure&#39;, &#39;Fantasy&#39;, &#39;Action&#39;] | 285 | 139.082615 | [&#39;United States of America&#39;] | 2007-05-19 | 2007 | 169.0 | [&#39;English&#39;] | Pirates of the Caribbean: At World&#39;s End | [&#39;Johnny Depp&#39;, &#39;Orlando Bloom&#39;, &#39;Keira Knight... | 6.9 | 4500 | . 7 | 280000000 | 1125403694 | 1405403694 | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Science Fiction&#39;] | 99861 | 134.279229 | [&#39;United States of America&#39;] | 2015-04-22 | 2015 | 141.0 | [&#39;English&#39;] | Avengers: Age of Ultron | [&#39;Robert Downey Jr.&#39;, &#39;Chris Hemsworth&#39;, &#39;Mark... | 7.3 | 6767 | . 10 | 270000000 | 121081192 | 391081192 | [&#39;Adventure&#39;, &#39;Fantasy&#39;, &#39;Action&#39;, &#39;Science Fi... | 1452 | 57.925623 | [&#39;United States of America&#39;] | 2006-06-28 | 2006 | 154.0 | [&#39;English&#39;, &#39;Français&#39;, &#39;Deutsch&#39;] | Superman Returns | [&#39;Brandon Routh&#39;, &#39;Kevin Spacey&#39;, &#39;Kate Boswor... | 5.4 | 1400 | . 4 | 260000000 | 24139100 | 284139100 | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Science Fiction&#39;] | 49529 | 43.926995 | [&#39;United States of America&#39;] | 2012-03-07 | 2012 | 132.0 | [&#39;English&#39;] | John Carter | [&#39;Taylor Kitsch&#39;, &#39;Lynn Collins&#39;, &#39;Samantha Mo... | 6.1 | 2124 | . # Comparison between min and max profits find_min_max_in(&#39;profit&#39;) . 0 13 . budget | 237000000 | 255000000 | . profit | 2550965087 | -165710090 | . revenue | 2787965087 | 89289910 | . genres | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Fantasy&#39;, &#39;Science Fi... | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Western&#39;] | . id | 19995 | 57201 | . popularity | 150.438 | 49.047 | . production_countries | [&#39;United States of America&#39;, &#39;United Kingdom&#39;] | [&#39;United States of America&#39;] | . release_date | 2009-12-10 00:00:00 | 2013-07-03 00:00:00 | . release_year | 2009 | 2013 | . runtime | 162 | 149 | . spoken_languages | [&#39;English&#39;, &#39;Español&#39;] | [&#39;English&#39;] | . title | Avatar | The Lone Ranger | . cast | [&#39;Sam Worthington&#39;, &#39;Zoe Saldana&#39;, &#39;Sigourney ... | [&#39;Johnny Depp&#39;, &#39;Armie Hammer&#39;, &#39;William Ficht... | . vote_average | 7.2 | 5.9 | . vote_count | 11800 | 2311 | . # to find the most talked about movies, we can sort the dataframe on the popularity column popular_movies_df = movies_df.sort_values(by =&#39;popularity&#39;, ascending=False).head() popular_movies_df.head() . budget profit revenue genres id popularity production_countries release_date release_year runtime spoken_languages title cast vote_average vote_count . 546 | 74000000 | 1082730962 | 1156730962 | [&#39;Family&#39;, &#39;Animation&#39;, &#39;Adventure&#39;, &#39;Comedy&#39;] | 211672 | 875.581305 | [&#39;United States of America&#39;] | 2015-06-17 | 2015 | 91.0 | [&#39;English&#39;] | Minions | [&#39;Sandra Bullock&#39;, &#39;Jon Hamm&#39;, &#39;Michael Keaton... | 6.4 | 4571 | . 95 | 165000000 | 510120017 | 675120017 | [&#39;Adventure&#39;, &#39;Drama&#39;, &#39;Science Fiction&#39;] | 157336 | 724.247784 | [&#39;Canada&#39;, &#39;United States of America&#39;, &#39;United... | 2014-11-05 | 2014 | 169.0 | [&#39;English&#39;] | Interstellar | [&#39;Matthew McConaughey&#39;, &#39;Jessica Chastain&#39;, &#39;A... | 8.1 | 10867 | . 788 | 58000000 | 725112979 | 783112979 | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Comedy&#39;] | 293660 | 514.569956 | [&#39;United States of America&#39;] | 2016-02-09 | 2016 | 108.0 | [&#39;English&#39;] | Deadpool | [&#39;Ryan Reynolds&#39;, &#39;Morena Baccarin&#39;, &#39;Ed Skrei... | 7.4 | 10995 | . 94 | 170000000 | 603328629 | 773328629 | [&#39;Action&#39;, &#39;Science Fiction&#39;, &#39;Adventure&#39;] | 118340 | 481.098624 | [&#39;United Kingdom&#39;, &#39;United States of America&#39;] | 2014-07-30 | 2014 | 121.0 | [&#39;English&#39;] | Guardians of the Galaxy | [&#39;Chris Pratt&#39;, &#39;Zoe Saldana&#39;, &#39;Dave Bautista&#39;... | 7.9 | 9742 | . 127 | 150000000 | 228858340 | 378858340 | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Science Fiction&#39;, &#39;Th... | 76341 | 434.278564 | [&#39;Australia&#39;, &#39;United States of America&#39;] | 2015-05-13 | 2015 | 120.0 | [&#39;English&#39;] | Mad Max: Fury Road | [&#39;Tom Hardy&#39;, &#39;Charlize Theron&#39;, &#39;Nicholas Hou... | 7.2 | 9427 | . # in terms of popularity score find_min_max_in(&#39;popularity&#39;).head() . 546 3921 . budget | 74000000 | 3000000 | . profit | 1082730962 | 3804016 | . revenue | 1156730962 | 6804016 | . genres | [&#39;Family&#39;, &#39;Animation&#39;, &#39;Adventure&#39;, &#39;Comedy&#39;] | [&#39;Drama&#39;] | . id | 211672 | 44634 | . Answer to question #3 . # in terms of runtime find_min_max_in(&#39;runtime&#39;) . 2384 3626 . budget | 18000000 | 5000000 | . profit | -17128721 | -925977 | . revenue | 871279 | 4074023 | . genres | [&#39;Crime&#39;, &#39;Drama&#39;, &#39;Thriller&#39;, &#39;History&#39;] | [&#39;Documentary&#39;] | . id | 43434 | 78394 | . popularity | 1.13838 | 4.49837 | . production_countries | [&#39;France&#39;, &#39;Germany&#39;] | [&#39;France&#39;, &#39;United Kingdom&#39;] | . release_date | 2010-05-19 00:00:00 | 2010-05-14 00:00:00 | . release_year | 2010 | 2010 | . runtime | 338 | 41 | . spoken_languages | [&#39;Français&#39;, &#39;Deutsch&#39;, &#39;English&#39;, &#39;Español&#39;, ... | [&#39;English&#39;] | . title | Carlos | Sea Rex 3D: Journey to a Prehistoric World | . cast | [&#39;Edgar Ramírez&#39;, &#39;Alexander Scheer&#39;, &#39;Fadi Ab... | [&#39;Guillaume Denaiffe&#39;, &#39;Norbert Ferrer&#39;, &#39;Chlo... | . vote_average | 6.7 | 5.9 | . vote_count | 50 | 11 | . # Average runtime of movies movies_df[&#39;runtime&#39;].mean() . 110.72437287085785 . Answer to question #4 . # movies rated above 7 movies_df[movies_df[&#39;vote_average&#39;] &gt;= 7.0] . budget profit revenue genres id popularity production_countries release_date release_year runtime spoken_languages title cast vote_average vote_count . 0 | 237000000 | 2550965087 | 2787965087 | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Fantasy&#39;, &#39;Science Fi... | 19995 | 150.437577 | [&#39;United States of America&#39;, &#39;United Kingdom&#39;] | 2009-12-10 | 2009 | 162.0 | [&#39;English&#39;, &#39;Español&#39;] | Avatar | [&#39;Sam Worthington&#39;, &#39;Zoe Saldana&#39;, &#39;Sigourney ... | 7.2 | 11800 | . 3 | 250000000 | 834939099 | 1084939099 | [&#39;Action&#39;, &#39;Crime&#39;, &#39;Drama&#39;, &#39;Thriller&#39;] | 49026 | 112.312950 | [&#39;United States of America&#39;] | 2012-07-16 | 2012 | 165.0 | [&#39;English&#39;] | The Dark Knight Rises | [&#39;Christian Bale&#39;, &#39;Michael Caine&#39;, &#39;Gary Oldm... | 7.6 | 9106 | . 6 | 260000000 | 331794936 | 591794936 | [&#39;Animation&#39;, &#39;Family&#39;] | 38757 | 48.681969 | [&#39;United States of America&#39;] | 2010-11-24 | 2010 | 100.0 | [&#39;English&#39;] | Tangled | [&#39;Zachary Levi&#39;, &#39;Mandy Moore&#39;, &#39;Donna Murphy&#39;... | 7.4 | 3330 | . 7 | 280000000 | 1125403694 | 1405403694 | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Science Fiction&#39;] | 99861 | 134.279229 | [&#39;United States of America&#39;] | 2015-04-22 | 2015 | 141.0 | [&#39;English&#39;] | Avengers: Age of Ultron | [&#39;Robert Downey Jr.&#39;, &#39;Chris Hemsworth&#39;, &#39;Mark... | 7.3 | 6767 | . 8 | 250000000 | 683959197 | 933959197 | [&#39;Adventure&#39;, &#39;Fantasy&#39;, &#39;Family&#39;] | 767 | 98.885637 | [&#39;United Kingdom&#39;, &#39;United States of America&#39;] | 2009-07-07 | 2009 | 153.0 | [&#39;English&#39;] | Harry Potter and the Half-Blood Prince | [&#39;Daniel Radcliffe&#39;, &#39;Rupert Grint&#39;, &#39;Emma Wat... | 7.4 | 5293 | . ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 4713 | 160000 | 6546368 | 6706368 | [&#39;Documentary&#39;, &#39;History&#39;] | 1779 | 3.284903 | [&#39;United States of America&#39;] | 1989-09-01 | 1989 | 91.0 | [&#39;English&#39;] | Roger &amp; Me | [&#39;Michael Moore&#39;, &#39;Roger B. Smith&#39;, &#39;Rhonda Br... | 7.4 | 90 | . 4724 | 10000 | 6990000 | 7000000 | [&#39;Drama&#39;, &#39;Fantasy&#39;, &#39;Horror&#39;, &#39;Science Fiction&#39;] | 985 | 20.399578 | [&#39;United States of America&#39;] | 1977-03-19 | 1977 | 89.0 | [&#39;English&#39;] | Eraserhead | [&#39;Jack Nance&#39;, &#39;Charlotte Stewart&#39;, &#39;Allen Jos... | 7.5 | 485 | . 4738 | 60000 | 3161152 | 3221152 | [&#39;Mystery&#39;, &#39;Drama&#39;, &#39;Thriller&#39;] | 473 | 27.788067 | [&#39;United States of America&#39;] | 1998-07-10 | 1998 | 84.0 | [&#39;English&#39;] | Pi | [&#39;Sean Gullette&#39;, &#39;Mark Margolis&#39;, &#39;Ben Shenkm... | 7.1 | 586 | . 4773 | 27000 | 3124130 | 3151130 | [&#39;Comedy&#39;] | 2292 | 19.748658 | [&#39;United States of America&#39;] | 1994-09-13 | 1994 | 92.0 | [&#39;English&#39;] | Clerks | [&quot;Brian O&#39;Halloran&quot;, &#39;Jeff Anderson&#39;, &#39;Jason M... | 7.4 | 755 | . 4792 | 20000 | 79000 | 99000 | [&#39;Crime&#39;, &#39;Horror&#39;, &#39;Mystery&#39;, &#39;Thriller&#39;] | 36095 | 0.212443 | [&#39;Japan&#39;] | 1997-11-06 | 1997 | 111.0 | [&#39;日本語&#39;] | Cure | [&#39;Koji Yakusho&#39;, &#39;Masato Hagiwara&#39;, &#39;Tsuyoshi ... | 7.4 | 63 | . 768 rows × 15 columns . Answer to question #5 . # Year we had the most number of profitable movies. # we&#39;ll first have to define a profitable movies #plotting a histogram of runtime of movies #giving the figure size(width, height) plt.figure(figsize=(9,5), dpi = 100) #On x-axis plt.xlabel(&#39;Runtime of the Movies&#39;, fontsize = 15) #On y-axis plt.ylabel(&#39;Nos.of Movies in the Dataset&#39;, fontsize=15) #Name of the graph plt.title(&#39;Runtime of all the movies&#39;, fontsize=15) #giving a histogram plot plt.hist(movies_df[&#39;runtime&#39;], rwidth = 0.9, bins =35) #displays the plot plt.show() . profits_year = movies_df.groupby(&#39;release_year&#39;)[&#39;profit&#39;].sum() #figure size(width, height) plt.figure(figsize=(12,6), dpi = 130) #on x-axis plt.xlabel(&#39;Release Year of Movies in the data set&#39;, fontsize = 12) #on y-axis plt.ylabel(&#39;Profits earned by Movies&#39;, fontsize = 12) #title of the line plot plt.title(&#39;Representing Total Profits earned by all movies Vs Year of their release.&#39;) #plotting the graph plt.plot(profits_year) #displaying the line plot plt.show() . # Most profitable year from the given dataset. profits_year.idxmax() . 2014 . #selecting the movies having profit $50M or more profit_data = movies_df[movies_df[&#39;profit&#39;] &gt;= 50000000] #reindexing new data profit_data.index = range(len(profit_data)) #we will start from 1 instead of 0 profit_data.index = profit_data.index + 1 #printing the changed dataset profit_data.head(3) . budget profit revenue genres id popularity production_countries release_date release_year runtime spoken_languages title cast vote_average vote_count . 1 | 237000000 | 2550965087 | 2787965087 | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Fantasy&#39;, &#39;Science Fi... | 19995 | 150.437577 | [&#39;United States of America&#39;, &#39;United Kingdom&#39;] | 2009-12-10 | 2009 | 162.0 | [&#39;English&#39;, &#39;Español&#39;] | Avatar | [&#39;Sam Worthington&#39;, &#39;Zoe Saldana&#39;, &#39;Sigourney ... | 7.2 | 11800 | . 2 | 300000000 | 661000000 | 961000000 | [&#39;Adventure&#39;, &#39;Fantasy&#39;, &#39;Action&#39;] | 285 | 139.082615 | [&#39;United States of America&#39;] | 2007-05-19 | 2007 | 169.0 | [&#39;English&#39;] | Pirates of the Caribbean: At World&#39;s End | [&#39;Johnny Depp&#39;, &#39;Orlando Bloom&#39;, &#39;Keira Knight... | 6.9 | 4500 | . 3 | 245000000 | 635674609 | 880674609 | [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Crime&#39;] | 206647 | 107.376788 | [&#39;United Kingdom&#39;, &#39;United States of America&#39;] | 2015-10-26 | 2015 | 148.0 | [&#39;Français&#39;, &#39;English&#39;, &#39;Español&#39;, &#39;Italiano&#39;,... | Spectre | [&#39;Daniel Craig&#39;, &#39;Christoph Waltz&#39;, &#39;Léa Seydo... | 6.3 | 4466 | . Answer to secondary question #1 . # formatting the data in the genres columns. profit_data[&#39;genres&#39;]=profit_data[&#39;genres&#39;].str.strip(&#39;[]&#39;).str.replace(&#39; &#39;,&#39;&#39;).str.replace(&quot;&#39;&quot;,&#39;&#39;) profit_data[&#39;genres&#39;]=profit_data[&#39;genres&#39;].str.split(&#39;,&#39;) profit_data.head() . budget profit revenue genres id popularity production_countries release_date release_year runtime spoken_languages title cast vote_average vote_count . 1 | 237000000 | 2550965087 | 2787965087 | [Action, Adventure, Fantasy, ScienceFiction] | 19995 | 150.437577 | [&#39;United States of America&#39;, &#39;United Kingdom&#39;] | 2009-12-10 | 2009 | 162.0 | [&#39;English&#39;, &#39;Español&#39;] | Avatar | [&#39;Sam Worthington&#39;, &#39;Zoe Saldana&#39;, &#39;Sigourney ... | 7.2 | 11800 | . 2 | 300000000 | 661000000 | 961000000 | [Adventure, Fantasy, Action] | 285 | 139.082615 | [&#39;United States of America&#39;] | 2007-05-19 | 2007 | 169.0 | [&#39;English&#39;] | Pirates of the Caribbean: At World&#39;s End | [&#39;Johnny Depp&#39;, &#39;Orlando Bloom&#39;, &#39;Keira Knight... | 6.9 | 4500 | . 3 | 245000000 | 635674609 | 880674609 | [Action, Adventure, Crime] | 206647 | 107.376788 | [&#39;United Kingdom&#39;, &#39;United States of America&#39;] | 2015-10-26 | 2015 | 148.0 | [&#39;Français&#39;, &#39;English&#39;, &#39;Español&#39;, &#39;Italiano&#39;,... | Spectre | [&#39;Daniel Craig&#39;, &#39;Christoph Waltz&#39;, &#39;Léa Seydo... | 6.3 | 4466 | . 4 | 250000000 | 834939099 | 1084939099 | [Action, Crime, Drama, Thriller] | 49026 | 112.312950 | [&#39;United States of America&#39;] | 2012-07-16 | 2012 | 165.0 | [&#39;English&#39;] | The Dark Knight Rises | [&#39;Christian Bale&#39;, &#39;Michael Caine&#39;, &#39;Gary Oldm... | 7.6 | 9106 | . 5 | 258000000 | 632871626 | 890871626 | [Fantasy, Action, Adventure] | 559 | 115.699814 | [&#39;United States of America&#39;] | 2007-05-01 | 2007 | 139.0 | [&#39;English&#39;, &#39;Français&#39;] | Spider-Man 3 | [&#39;Tobey Maguire&#39;, &#39;Kirsten Dunst&#39;, &#39;James Fran... | 5.9 | 3576 | . # plt.subplots(figsize=(12,10)) list1=[] # extending the list of genres to collect all the genres of all the profitable movies for i in profit_data[&#39;genres&#39;]: list1.extend(i) ax = pd.Series(list1).value_counts()[:10].sort_values(ascending=True).plot.barh( width=0.9, color=sns.color_palette(&#39;summer_r&#39;,10)) ax . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1a2dac10&gt; . plt.subplots(figsize=(12,10)) list1=[] # extending the list of genres to collect all the genres of all the profitable movies for i in profit_data[&#39;genres&#39;]: list1.extend(i) ax = pd.Series(list1).value_counts()[:10].sort_values(ascending=True).plot.barh( width=0.9, color=sns.color_palette(&#39;summer_r&#39;,10)) for i, v in enumerate(pd.Series(list1).value_counts()[:10].sort_values(ascending=True).values): ax.text(.8, i, v,fontsize=12,color=&#39;white&#39;,weight=&#39;bold&#39;) ax.patches[9].set_facecolor(&#39;r&#39;) plt.title(&#39;Top Genres&#39;) plt.show() . Comedy looks like the most preferred option to make profitable movies. . Most frequent cast . Let&#39;s try to find out the most frequent cast in the movies based on which we can tell about the success factor of the cast. . profit_data[&#39;cast&#39;]=profit_data[&#39;cast&#39;].str.strip(&#39;[]&#39;).str.replace(&#39; &#39;,&#39;&#39;).str.replace(&quot;&#39;&quot;,&#39;&#39;) profit_data[&#39;cast&#39;]=profit_data[&#39;cast&#39;].str.split(&#39;,&#39;) plt.subplots(figsize=(12,10)) list1=[] for i in profit_data[&#39;cast&#39;]: list1.extend(i) ax = pd.Series(list1).value_counts()[:10].sort_values(ascending=True).plot.barh(width=0.9,color=sns.color_palette(&#39;summer_r&#39;,10)) for i, v in enumerate(pd.Series(list1).value_counts()[:10].sort_values(ascending=True).values): ax.text(.8, i, v,fontsize=12,color=&#39;white&#39;,weight=&#39;bold&#39;) ax.patches[9].set_facecolor(&#39;r&#39;) plt.title(&#39;Top Cast&#39;) plt.show() . Samuel Jackson seems to be on top with 53 movies. . Answer to secondary question #3 . profit_data[&#39;profit&#39;].mean() profit_data[&#39;revenue&#39;].mean() . 262022090.17768925 . Answer to secondary question #4 . profit_data[&#39;runtime&#39;].mean() . 114.37928286852589 . Answer to secondary question #5 . profit_data[&#39;budget&#39;].mean() . 63032056.92111554 . Answer to secondary question #6 . profit_data[&#39;spoken_languages&#39;]=profit_data[&#39;spoken_languages&#39;].str.strip(&#39;[]&#39;).str.replace(&#39; &#39;,&#39;&#39;).str.replace(&quot;&#39;&quot;,&#39;&#39;) profit_data[&#39;spoken_languages&#39;]=profit_data[&#39;spoken_languages&#39;].str.split(&#39;,&#39;) plt.subplots(figsize=(12,10)) list1=[] for i in profit_data[&#39;spoken_languages&#39;]: list1.extend(i) ax = pd.Series(list1).value_counts()[:10].sort_values(ascending=True).plot.barh(width=0.9,color=sns.color_palette(&#39;summer_r&#39;,10)) for i, v in enumerate(pd.Series(list1).value_counts()[:10].sort_values(ascending=True).values): ax.text(.8, i, v,fontsize=12,color=&#39;white&#39;,weight=&#39;bold&#39;) ax.patches[9].set_facecolor(&#39;r&#39;) plt.title(&#39;Frequency of language used!&#39;) plt.show() . English seems to be the most profitable language. . Conclusion . This was a very interesting data analysis. We came out with some very interesting facts about movies. After this analysis we can conclude following:&gt; For a Movie to be in successful criteria&gt; Average Budget must be around 63 millon dollar Average duration of the movie must be 114 minutes Any one of these should be in the cast :Samuel Jackson, Robert De Neiro, Morgan Freeman, Bruce WillisGenre must be : Action, Adventure, Thriller, Comedy, Drama. By doing all this the movie might be one of the hits and hence can earn an average revenue of around 262 million dollar. . Tutorial #2: Descriptive statistics . #Create a Dictionary of series d = {&#39;Name&#39;:pd.Series([&#39;Tom&#39;,&#39;James&#39;,&#39;Ricky&#39;,&#39;Vin&#39;,&#39;Steve&#39;,&#39;Smith&#39;,&#39;Jack&#39;, &#39;Lee&#39;,&#39;David&#39;,&#39;Gasper&#39;,&#39;Betina&#39;,&#39;Andres&#39;]), &#39;Age&#39;:pd.Series([25,26,25,23,30,29,23,34,40,30,51,46]), &#39;Rating&#39;:pd.Series([4.23,3.24,3.98,2.56,3.20,4.6,3.8,3.78,2.98,4.80,4.10,3.65]) } #Create a DataFrame df = pd.DataFrame(d) print(df) . Name Age Rating 0 Tom 25 4.23 1 James 26 3.24 2 Ricky 25 3.98 3 Vin 23 2.56 4 Steve 30 3.20 5 Smith 29 4.60 6 Jack 23 3.80 7 Lee 34 3.78 8 David 40 2.98 9 Gasper 30 4.80 10 Betina 51 4.10 11 Andres 46 3.65 . print(df.sum()) . Name TomJamesRickyVinSteveSmithJackLeeDavidGasperBe... Age 382 Rating 44.92 dtype: object . df.describe() . Age Rating . count | 12.000000 | 12.000000 | . mean | 31.833333 | 3.743333 | . std | 9.232682 | 0.661628 | . min | 23.000000 | 2.560000 | . 25% | 25.000000 | 3.230000 | . 50% | 29.500000 | 3.790000 | . 75% | 35.500000 | 4.132500 | . max | 51.000000 | 4.800000 | . df[[&#39;Age&#39;, &#39;Rating&#39;]].median() . Age 29.50 Rating 3.79 dtype: float64 . Kurtosis . df[[&#39;Age&#39;, &#39;Rating&#39;]].kurtosis() . Age 0.249310 Rating -0.487236 dtype: float64 . Skewness . df[[&#39;Age&#39;, &#39;Rating&#39;]].skew() . Age 1.135089 Rating -0.153629 dtype: float64 . Tutorial #3: Correlation test . # Importing required libraries import pandas as pd from scipy import stats df = pd.read_csv(&#39;../data/diamonds.csv&#39;) df[[&quot;carat&quot;, &quot;price&quot;, &quot;depth&quot;]].describe() . carat price depth . count | 53940.000000 | 53940.000000 | 53940.000000 | . mean | 0.797940 | 3932.799722 | 61.749405 | . std | 0.474011 | 3989.439738 | 1.432621 | . min | 0.200000 | 326.000000 | 43.000000 | . 25% | 0.400000 | 950.000000 | 61.000000 | . 50% | 0.700000 | 2401.000000 | 61.800000 | . 75% | 1.040000 | 5324.250000 | 62.500000 | . max | 5.010000 | 18823.000000 | 79.000000 | . df.plot.scatter(&quot;carat&quot;, &quot;price&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a26bbab90&gt; . It appears that there is a linear relationship present- as the carat increases so does the price. It’s a bit hard to tell because there is a shotgun shot looking spread that starts after 1 carat. This leads me to believe that we are violating the assumption of homoscedasticity between the variables. . Correlation Examples . 1. Pandas Correlation . df[&#39;carat&#39;].corr(df[&#39;price&#39;]) . 0.9215913011934779 . df[&#39;carat&#39;].corr(df[&#39;price&#39;], method= &#39;spearman&#39;) . 0.9628827988813001 . 2. Pearson Correlation . stats.pearsonr(df[&#39;carat&#39;], df[&#39;price&#39;]) . (0.921591301193477, 0.0) . stats.spearmanr(df[&#39;carat&#39;], df[&#39;price&#39;]) . SpearmanrResult(correlation=0.9628827988813001, pvalue=0.0) . stats.kendalltau(df[&#39;carat&#39;], df[&#39;price&#39;]) . KendalltauResult(correlation=0.8341049107108127, pvalue=0.0) . Population and Sample . # Create a Population DataFrame with 10 data data = pd.DataFrame() data[&#39;population&#39;] = [47, 48, 85, 20, 19, 13, 72, 16, 50, 60] . # Draw sample with replacement, size=5 from Population a_sample_with_replacement = data[&#39;population&#39;].sample(5, replace=True) print(a_sample_with_replacement) . 0 47 5 13 8 50 2 85 3 20 Name: population, dtype: int64 . # Draw sample without replacement, size=5 from Population a_sample_without_replacement = data[&#39;population&#39;].sample(5, replace=False) print(a_sample_without_replacement) . 1 48 2 85 3 20 7 16 6 72 Name: population, dtype: int64 . # Calculate mean and variance population_mean = data[&#39;population&#39;].mean() population_var = data[&#39;population&#39;].var(ddof=0) print(&#39;Population mean is &#39;, population_mean) print(&#39;Population variance is&#39;, population_var) . Population mean is 43.0 Population variance is 571.8 . # Calculate sample mean and sample standard deviation, size =10 # You will get different mean and varince every time when you excecute the below code a_sample = data[&#39;population&#39;].sample(10, replace=True) sample_mean = a_sample.mean() sample_var = a_sample.var() print(&#39;Sample mean is &#39;, sample_mean) print(&#39;Sample variance is&#39;, sample_var) . Sample mean is 48.8 Sample variance is 652.6222222222223 . Distribution of sample mean . meanlist = [] for t in range(10000): sample = pd.DataFrame(np.random.normal(10, 5, size=30)) meanlist.append(sample[0].mean()) . collection = pd.DataFrame() collection[&#39;meanlist&#39;] = meanlist . collection[&#39;meanlist&#39;].hist(bins=100, normed=1,figsize=(15,8)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a249eb250&gt; . from scipy.stats import norm ms = pd.read_csv(&#39;../data/msft_eod.csv&#39;) ms.head() . Date Open High Low Close Volume Dividend Split Adj_Open Adj_High Adj_Low Adj_Close Adj_Volume . 0 | 2010-01-04 | 30.62 | 31.10 | 30.59 | 30.950 | 38409100.0 | 0.0 | 1.0 | 24.356481 | 24.738294 | 24.332618 | 24.618978 | 38409100.0 | . 1 | 2010-01-05 | 30.85 | 31.10 | 30.64 | 30.960 | 49749600.0 | 0.0 | 1.0 | 24.539433 | 24.738294 | 24.372390 | 24.626932 | 49749600.0 | . 2 | 2010-01-06 | 30.88 | 31.08 | 30.52 | 30.770 | 58182400.0 | 0.0 | 1.0 | 24.563297 | 24.722385 | 24.276937 | 24.475798 | 58182400.0 | . 3 | 2010-01-07 | 30.63 | 30.70 | 30.19 | 30.452 | 50559700.0 | 0.0 | 1.0 | 24.364436 | 24.420117 | 24.014440 | 24.222847 | 50559700.0 | . 4 | 2010-01-08 | 30.28 | 30.88 | 30.24 | 30.660 | 51197400.0 | 0.0 | 1.0 | 24.086030 | 24.563297 | 24.054213 | 24.388299 | 51197400.0 | . # we will use log return for average stock return of Microsoft ms[&#39;logReturn&#39;] = np.log(ms[&#39;Close&#39;].shift(-1)) - np.log(ms[&#39;Close&#39;]) . # Lets build 90% confidence interval for log return sample_size = ms[&#39;logReturn&#39;].shape[0] sample_mean = ms[&#39;logReturn&#39;].mean() sample_std = ms[&#39;logReturn&#39;].std(ddof=1) / sample_size**0.5 # left and right quantile z_left = norm.ppf(0.1) z_right = norm.ppf(0.9) # upper and lower bound interval_left = sample_mean +z_left*sample_std interval_right = sample_mean +z_right*sample_std . # 90% confidence interval tells you that there will be 90% chance that the average stock return lies between &quot;interval_left&quot; # and &quot;interval_right&quot;. print(&#39;90% confidence interval is &#39;, (interval_left, interval_right)) . 90% confidence interval is (0.00018024329540852746, 0.0009571593069395294) . Hypothesis Testing . # Log return goes up and down during the period ms[&#39;logReturn&#39;].plot(figsize=(20, 8)) plt.axhline(0, color=&#39;red&#39;) plt.show() . Steps involved in testing a claim by hypothesis testing . Step 1: Set hypothesis . $H_0 : mu = 0$ $H_a : mu neq 0$ . H0 means the average stock return is 0 H1 means the average stock return is not equal to 0 . Step 2: Calculate test statistic . sample_mean = ms[&#39;logReturn&#39;].mean() sample_std = ms[&#39;logReturn&#39;].std(ddof=1) n = ms[&#39;logReturn&#39;].shape[0] # if sample size n is large enough, we can use z-distribution, instead of t-distribtuion # mu = 0 under the null hypothesis zhat = (sample_mean - 0)/(sample_std/n**0.5) print(zhat) . 1.876187469506785 . Step 3: Set desicion criteria . # confidence level alpha = 0.05 zleft = norm.ppf(alpha/2, 0, 1) zright = -zleft # z-distribution is symmetric print(zleft, zright) . -1.9599639845400545 1.9599639845400545 . Step 4: Make decision - shall we reject H0? . print(&#39;At significant level of {}, shall we reject: {}&#39;.format(alpha, zhat&gt;zright or zhat&lt;zleft)) . At significant level of 0.05, shall we reject: False . An alternative method: p-value . # step 3 (p-value) p = 1 - norm.cdf(zhat, 0, 1) print(p) . 0.030314771044210875 . # step 4 print(&#39;At significant level of {}, shall we reject: {}&#39;.format(alpha, p &lt; alpha)) . At significant level of 0.05, shall we reject: True .",
            "url": "https://ishmaelasabere.github.io/Data-Science/2020/08/25/Statistical-Analytics-with-Movies-Dataset.html",
            "relUrl": "/2020/08/25/Statistical-Analytics-with-Movies-Dataset.html",
            "date": " • Aug 25, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fraud Analytics using Python",
            "content": "import pandas as pd . # reading the data df = pd.read_csv(&quot;../data/creditcard.csv&quot;) df.head() # to look at the first few rows of the dataframe . Time V1 V2 V3 V4 V5 V6 V7 V8 V9 ... V21 V22 V23 V24 V25 V26 V27 V28 Amount Class . 0 | 0.0 | -1.359807 | -0.072781 | 2.536347 | 1.378155 | -0.338321 | 0.462388 | 0.239599 | 0.098698 | 0.363787 | ... | -0.018307 | 0.277838 | -0.110474 | 0.066928 | 0.128539 | -0.189115 | 0.133558 | -0.021053 | 149.62 | 0 | . 1 | 0.0 | 1.191857 | 0.266151 | 0.166480 | 0.448154 | 0.060018 | -0.082361 | -0.078803 | 0.085102 | -0.255425 | ... | -0.225775 | -0.638672 | 0.101288 | -0.339846 | 0.167170 | 0.125895 | -0.008983 | 0.014724 | 2.69 | 0 | . 2 | 1.0 | -1.358354 | -1.340163 | 1.773209 | 0.379780 | -0.503198 | 1.800499 | 0.791461 | 0.247676 | -1.514654 | ... | 0.247998 | 0.771679 | 0.909412 | -0.689281 | -0.327642 | -0.139097 | -0.055353 | -0.059752 | 378.66 | 0 | . 3 | 1.0 | -0.966272 | -0.185226 | 1.792993 | -0.863291 | -0.010309 | 1.247203 | 0.237609 | 0.377436 | -1.387024 | ... | -0.108300 | 0.005274 | -0.190321 | -1.175575 | 0.647376 | -0.221929 | 0.062723 | 0.061458 | 123.50 | 0 | . 4 | 2.0 | -1.158233 | 0.877737 | 1.548718 | 0.403034 | -0.407193 | 0.095921 | 0.592941 | -0.270533 | 0.817739 | ... | -0.009431 | 0.798278 | -0.137458 | 0.141267 | -0.206010 | 0.502292 | 0.219422 | 0.215153 | 69.99 | 0 | . 5 rows × 31 columns . # Explore the features available in your dataframe df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 284807 entries, 0 to 284806 Data columns (total 31 columns): Time 284807 non-null float64 V1 284807 non-null float64 V2 284807 non-null float64 V3 284807 non-null float64 V4 284807 non-null float64 V5 284807 non-null float64 V6 284807 non-null float64 V7 284807 non-null float64 V8 284807 non-null float64 V9 284807 non-null float64 V10 284807 non-null float64 V11 284807 non-null float64 V12 284807 non-null float64 V13 284807 non-null float64 V14 284807 non-null float64 V15 284807 non-null float64 V16 284807 non-null float64 V17 284807 non-null float64 V18 284807 non-null float64 V19 284807 non-null float64 V20 284807 non-null float64 V21 284807 non-null float64 V22 284807 non-null float64 V23 284807 non-null float64 V24 284807 non-null float64 V25 284807 non-null float64 V26 284807 non-null float64 V27 284807 non-null float64 V28 284807 non-null float64 Amount 284807 non-null float64 Class 284807 non-null int64 dtypes: float64(30), int64(1) memory usage: 67.4 MB . Data . The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,315 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions. . It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, the original features and more background information about the data can&#39;t be revealed. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are &#39;Time&#39; and &#39;Amount&#39;. Feature &#39;Time&#39; contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature &#39;Amount&#39; is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature &#39;Class&#39; is the response variable and it takes value 1 in case of fraud and 0 otherwise. . Let&#39;s look at the class labels . # Counting the occurrences of fraud and no fraud and print them occ = df[&#39;Class&#39;].value_counts() occ . 0 284315 1 492 Name: Class, dtype: int64 . # print the ratio of fraud cases print(occ / len(df)) . 0 0.998273 1 0.001727 Name: Class, dtype: float64 . Visualizing the data . In this exercise, you&#39;ll look at the data and visualize the fraud to non-fraud ratio. It is always a good starting point in your fraud analysis, to look at your data first, before you make any changes to it. . import numpy as np import matplotlib.pyplot as plt %matplotlib inline count_classes = pd.value_counts(df[&#39;Class&#39;], sort = True).sort_index() count_classes.plot(kind = &#39;bar&#39;) plt.title(&quot;Fraud class histogram&quot;) plt.xlabel(&quot;Class&quot;) plt.ylabel(&quot;Frequency&quot;) . Text(0, 0.5, &#39;Frequency&#39;) . def prep_data(df): &quot;&quot;&quot; Prepare the data to train the model args: df - dataframe returns: X - array of columns y - class array to be predicted &quot;&quot;&quot; X = df.iloc[:, 1:29] X = np.array(X).astype(np.float) # features array and converting the values to float y = df.iloc[:, 30] y = np.array(y).astype(np.float) # target variable - class return X,y def plot_data(X, y): plt.scatter(X[y == 0, 0], X[y == 0, 1], label=&quot;Class #0&quot;, alpha=0.5, linewidth=0.15) plt.scatter(X[y == 1, 0], X[y == 1, 1], label=&quot;Class #1&quot;, alpha=0.5, linewidth=0.15, c=&#39;r&#39;) plt.legend() return plt.show() . # Create X and y from the prep_data function X, y = prep_data(df) # Plot our data by running our plot data function on X and y plot_data(X, y) . array([-1.35980713, 1.19185711, -1.35835406, ..., 1.91956501, -0.24044005, -0.53341252]) . df.iloc[0].count . &lt;bound method Series.count of Time 0.000000 V1 -1.359807 V2 -0.072781 V3 2.536347 V4 1.378155 V5 -0.338321 V6 0.462388 V7 0.239599 V8 0.098698 V9 0.363787 V10 0.090794 V11 -0.551600 V12 -0.617801 V13 -0.991390 V14 -0.311169 V15 1.468177 V16 -0.470401 V17 0.207971 V18 0.025791 V19 0.403993 V20 0.251412 V21 -0.018307 V22 0.277838 V23 -0.110474 V24 0.066928 V25 0.128539 V26 -0.189115 V27 0.133558 V28 -0.021053 Amount 149.620000 Class 0.000000 Name: 0, dtype: float64&gt; . df.shape . (284807, 31) . How to manage imbalanced data? . Let&#39;s learn different methods which can help us deal with imbalanced data. There are various resmapling techniques to handle imbalanced data: . Random Under Sampling(RUS): This reduces the majority class and makes the data balanced. | Random Over Sampling(ROS): This generated duplicates of the minority class. Inefficient because of duplicacy. | Synthetic Minority Oversampling Technique(SMOTE): Generates fake realistic data to balance out the data. | . We are going to use the best possible option which is SMOTE here. . from imblearn.over_sampling import SMOTE, RandomOverSampler from imblearn.under_sampling import RandomUnderSampler X, y = prep_data(df) # preparing the data method = SMOTE() # invoking the resampling method X_resampled, y_resampled = method.fit_sample(X,y) # creating the resampled feature set plot_data(X_resampled, y_resampled) . pd.value_counts(y_resampled) . 1.0 492 0.0 492 dtype: int64 . def compare_plot(X,y,X_resampled,y_resampled, method): # Start a plot figure f, (ax1, ax2) = plt.subplots(1, 2) # sub-plot number 1, this is our normal data c0 = ax1.scatter(X[y == 0, 0], X[y == 0, 1], label=&quot;Class #0&quot;,alpha=0.5) c1 = ax1.scatter(X[y == 1, 0], X[y == 1, 1], label=&quot;Class #1&quot;,alpha=0.5, c=&#39;r&#39;) ax1.set_title(&#39;Original set&#39;) # sub-plot number 2, this is our oversampled data ax2.scatter(X_resampled[y_resampled == 0, 0], X_resampled[y_resampled == 0, 1], label=&quot;Class #0&quot;, alpha=.5) ax2.scatter(X_resampled[y_resampled == 1, 0], X_resampled[y_resampled == 1, 1], label=&quot;Class #1&quot;, alpha=.5,c=&#39;r&#39;) ax2.set_title(method) # some settings and ready to go plt.figlegend((c0, c1), (&#39;Class #0&#39;, &#39;Class #1&#39;), loc=&#39;lower center&#39;, ncol=2, labelspacing=0.) plt.tight_layout(pad=3) return plt.show() . # Print the value_counts on the original labels y print(pd.value_counts(pd.Series(y))) # Print the value_counts print(pd.value_counts(pd.Series(y_resampled))) # Run compare_plot compare_plot(X, y, X_resampled, y_resampled, method=&#39;SMOTE&#39;) . 0.0 284315 1.0 492 dtype: int64 1.0 284315 0.0 284315 dtype: int64 . Rule-based method to detect fraudsters . In this exercise you&#39;re going to try finding fraud cases in our credit card dataset the &quot;old way&quot;. First you&#39;ll define threshold values using common statistics, to split fraud and non-fraud. Then, use those thresholds on your features to detect fraud. This is common practice within fraud analytics teams. . Statistical thresholds are often determined by looking at the mean values of observations. Let&#39;s start this exercise by checking whether feature means differ between fraud and non-fraud cases. Then, you&#39;ll use that information to create common sense thresholds. Finally, you&#39;ll check how well this performs in fraud detection. . # Get the mean for each group df.groupby(&#39;Class&#39;).mean() . Time V1 V2 V3 V4 V5 V6 V7 V8 V9 ... V20 V21 V22 V23 V24 V25 V26 V27 V28 Amount . Class . 0 94838.202258 | 0.008258 | -0.006271 | 0.012171 | -0.007860 | 0.005453 | 0.002419 | 0.009637 | -0.000987 | 0.004467 | ... | -0.000644 | -0.001235 | -0.000024 | 0.000070 | 0.000182 | -0.000072 | -0.000089 | -0.000295 | -0.000131 | 88.291022 | . 1 80746.806911 | -4.771948 | 3.623778 | -7.033281 | 4.542029 | -3.151225 | -1.397737 | -5.568731 | 0.570636 | -2.581123 | ... | 0.372319 | 0.713588 | 0.014049 | -0.040308 | -0.105130 | 0.041449 | 0.051648 | 0.170575 | 0.075667 | 122.211321 | . 2 rows × 30 columns . # Implement a rule for stating which cases are flagged as fraud df[&#39;flag_as_fraud&#39;] = np.where(np.logical_and(df[&#39;V1&#39;] &lt; -3, df[&#39;V3&#39;] &lt; -5), 1, 0) # Create a crosstab of flagged fraud cases versus the actual fraud cases pd.crosstab(df.Class, df.flag_as_fraud, rownames=[&#39;Actual Fraud&#39;], colnames=[&#39;Flagged Fraud&#39;]) . Flagged Fraud 0 1 . Actual Fraud . 0 283089 | 1226 | . 1 322 | 170 | . not bad, with this rule, we detect 170 out of 492 fraud cases, but can&#39;t detect the other 322, and get 1226 false positives. . Now, using ML classification to catch fraudsters . In this exercise you&#39;ll see what happens when you use a simple machine learning model on our credit card data instead. . Do you think you can beat those results? Remember, you&#39;ve predicted 170 out of 492 fraud cases, and had 1226 false positives. That&#39;s less than half of the cases caught, Also false positives were roughly 3 times the actual amount of fraud cases. . So with that in mind, let&#39;s implement a Logistic Regression model.[Poll] If not, you might want to refresh that at this point. But don&#39;t worry, you&#39;ll be guided through the structure of the machine learning model. . # importing sklearn for training splitting and importing the classifier from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix . # Create the training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0) # Fit a logistic regression model to our data model = LogisticRegression() model.fit(X_train, y_train) # Obtain model predictions predicted = model.predict(X_test) # Print the classifcation report and confusion matrix print(&#39;Classification report: n&#39;, classification_report(y_test, predicted)) conf_mat = confusion_matrix(y_true=y_test, y_pred=predicted) print(&#39;Confusion matrix: n&#39;, conf_mat) . /Users/harshit/projects/oreilly/or_env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning. FutureWarning) . Classification report: precision recall f1-score support 0.0 1.00 1.00 1.00 85296 1.0 0.89 0.62 0.73 147 micro avg 1.00 1.00 1.00 85443 macro avg 0.95 0.81 0.87 85443 weighted avg 1.00 1.00 1.00 85443 Confusion matrix: [[85285 11] [ 56 91]] . We are getting much less false positives, so that&#39;s an improvement. Also, we&#39;re catching a higher percentage of fraud cases, so that is also better than before. Do you understand why we have less observations to look at in the confusion matrix? Remember we are using only our test data to calculate the model results on. We&#39;re comparing the crosstab on the full dataset from the last exercise, with a confusion matrix of only 30% of the total dataset, so that&#39;s where that difference comes from . Logistic regression combined with SMOTE . In this exercise, we&#39;re going to take the Logistic Regression model from the previous exercise, and combine that with a SMOTE resampling method. We&#39;ll see how to do that efficiently by using a pipeline that combines the resampling method with the model in one go. First, we need to define the pipeline that we&#39;re going to use. . # This is the pipeline module we need for this from imblearn from imblearn.pipeline import Pipeline from imblearn.over_sampling import BorderlineSMOTE # Define which resampling method and which ML model to use in the pipeline resampling = BorderlineSMOTE(kind=&#39;borderline-2&#39;) # instead SMOTE(kind=&#39;borderline2&#39;)90 model = LogisticRegression() # Define the pipeline, tell it to combine SMOTE with the Logistic Regression model pipeline = Pipeline([(&#39;SMOTE&#39;, resampling), (&#39;Logistic Regression&#39;, model)]) . Now that you have our pipeline defined which is a combination of logistic regression with a SMOTE method, let&#39;s run it on the data. You can treat the pipeline as if it were a single machine learning model. Our data X and y are already defined, and the pipeline is defined in the previous exercise. Are you curious to find out what the model results are? Let&#39;s give it a try! . # Split your data X and y, into a training and a test set and fit the pipeline onto the training data X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.3, random_state=0) # Fit your pipeline onto your training set and obtain predictions by fitting the model onto the test data pipeline.fit(X_train, y_train) predicted = pipeline.predict(X_test) # Obtain the results from the classification report and confusion matrix print(&#39;Classifcation report: n&#39;, classification_report(y_test, predicted)) conf_mat = confusion_matrix(y_true=y_test, y_pred=predicted) print(&#39;Confusion matrix: n&#39;, conf_mat) . /Users/harshit/projects/oreilly/or_env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning. FutureWarning) . Classifcation report: precision recall f1-score support 0.0 1.00 0.98 0.99 85296 1.0 0.07 0.88 0.14 147 micro avg 0.98 0.98 0.98 85443 macro avg 0.54 0.93 0.56 85443 weighted avg 1.00 0.98 0.99 85443 Confusion matrix: [[83701 1595] [ 18 129]] . Whoops! As you can see, the SMOTE hasn&#39;t helped but has added more false positives in our results. We have a very high number of false positives. Remember, not in all cases does resampling necessarily lead to better results. When the fraud cases are very spread and scattered over the data, using SMOTE can introduce a bit of bias. Nearest neighbors aren&#39;t necessarily also fraud cases, so the synthetic samples might &#39;confuse&#39; the model slightly. . Fraud detection using labelled data . Now that you&#39;re familiar with the main challenges of fraud detection, you&#39;re about to learn how to flag fraudulent transactions with supervised learning. You will use classifiers, adjust them and compare them to find the most efficient fraud detection model. . Natural hit rate . # Count the total number of observations from the length of y total_obs = len(y) # Count the total number of non-fraudulent observations non_fraud = [i for i in y if i == 0] count_non_fraud = non_fraud.count(0) # Calculate the percentage of non fraud observations in the dataset percentage = (float(count_non_fraud)/float(total_obs)) * 100 # Print the percentage: this is our &quot;natural accuracy&quot; by doing nothing print(percentage) . 99.82725143693798 . Part 1: Random forest classifier . # Import the random forest model from sklearn from sklearn.ensemble import RandomForestClassifier # Split your data into training and test set X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0) # Define the model as the random forest model = RandomForestClassifier(random_state=5) . X_test.shape, y_test.shape . ((85443, 28), (85443,)) . Part 2: Random forest classifier . # Fit the model to our training set model.fit(X_train, y_train) # Obtain predictions from the test data predicted = model.predict(X_test) . /Users/harshit/projects/oreilly/or_env/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22. &#34;10 in version 0.20 to 100 in 0.22.&#34;, FutureWarning) . from sklearn.metrics import accuracy_score # Print the accuracy performance metric print(accuracy_score(y_test, predicted)) . 0.9994733330992591 . Performance metrics for the RF model . With highly imbalanced fraud data, the AUROC curve is a more reliable performance metric, used to compare different classifiers. Moreover, the classification report tells you about the precision and recall of your model, whilst the confusion matrix actually shows how many fraud cases you can predict correctly . # Import the packages to get the different performance metrics from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score # Obtain the predictions from our random forest model predicted = model.predict(X_test) # Predict probabilities probs = model.predict_proba(X_test) # Print the AUCROC curve, classification report and confusion matrix print(classification_report(y_test, predicted)) print(confusion_matrix(y_test, predicted, labels=[0, 1])) print(&quot;AUC ROC score: &quot;, roc_auc_score(y_test, probs[:,1])) . precision recall f1-score support 0.0 1.00 1.00 1.00 85296 1.0 0.95 0.73 0.83 147 micro avg 1.00 1.00 1.00 85443 macro avg 0.97 0.87 0.91 85443 weighted avg 1.00 1.00 1.00 85443 [[85290 6] [ 39 108]] AUC ROC score: 0.9214738957860391 . We have now obtained more meaningful performance metrics that tell us how well the model performs, given the highly imbalanced data that you&#39;re working with. The model predicts 73 cases of fraud, out of which 72 are actual fraud. You have only 1 false positives. This is really good, and as a result you have a very high precision score. You do however, don&#39;t catch 19 cases of actual fraud (false negative). We consider the scores based on fraud detection (1). . probs . array([[1., 0.], [1., 0.], [1., 0.], ..., [1., 0.], [1., 0.], [1., 0.]]) . Plotting the Precision Recall Curve . In this curve Precision and Recall are inversely related; as Precision increases, Recall falls and vice-versa. . def plot_pr_curve(recall, precision, average_precision): plt.step(recall, precision, color=&#39;b&#39;, alpha=0.2, where=&#39;post&#39;) plt.fill_between(recall, precision, step=&#39;post&#39;, alpha=0.2, color=&#39;b&#39;) plt.xlabel(&#39;Recall&#39;) plt.ylabel(&#39;Precision&#39;) plt.ylim([0.0, 1.05]) plt.xlim([0.0, 1.0]) plt.title(&#39;2-class Precision-Recall curve: AP={0:0.2f}&#39;.format(average_precision)) plt.show() . from sklearn.metrics import average_precision_score, precision_recall_curve, roc_curve # Calculate average precision and the PR curve average_precision = average_precision_score(y_test, predicted) # Obtain precision and recall precision, recall, _ = precision_recall_curve(y_test, predicted) # Plot the recall precision tradeoff plot_pr_curve(recall, precision, average_precision) . The ROC curve plots the true positives vs. false positives , for a classifier, as its discrimination threshold is varied. Since, a random method describes a horizontal curve through the unit interval, it has an AUC of 0.5. Minimally, classifiers should perform better than this, and the extent to which they score higher than one another (meaning the area under the ROC curve is larger), they have better expected performance. . # Create true and false positive rates false_positive_rate, true_positive_rate, threshold = roc_curve(y_test, predicted) . false_positive_rate . array([0.00000000e+00, 7.03432752e-05, 1.00000000e+00]) . true_positive_rate . array([0. , 0.73469388, 1. ]) . threshold . array([2., 1., 0.]) . # Plot ROC curve plt.title(&quot;Receiver Operating Characteristic&quot;) plt.plot(false_positive_rate, true_positive_rate) plt.plot([0, 1], ls=&quot;--&quot;) plt.plot([0, 0], [1, 0] , c=&quot;.7&quot;), plt.plot([1, 1] , c=&quot;.7&quot;) plt.ylabel(&quot;True Positive Rate&quot;) plt.xlabel(&quot;False Positive Rate&quot;) plt.show() . # Define the model with balanced subsample model = RandomForestClassifier(class_weight=&#39;balanced_subsample&#39;, n_estimators=100, random_state=5)################################### # Fit your training model to your training set model.fit(X_train, y_train) # Obtain the predicted values and probabilities from the model predicted = model.predict(X_test) probs = model.predict_proba(X_test) # Print the roc_auc_score, the classification report and confusion matrix print(roc_auc_score(y_test, probs[:,1])) print(classification_report(y_test, predicted)) print(confusion_matrix(y_test, predicted)) . 0.9442655157166974 precision recall f1-score support 0.0 1.00 1.00 1.00 85296 1.0 0.96 0.74 0.84 147 micro avg 1.00 1.00 1.00 85443 macro avg 0.98 0.87 0.92 85443 weighted avg 1.00 1.00 1.00 85443 [[85292 4] [ 38 109]] . we can see that the model results don&#39;t improve drastically. If we mostly care about catching fraud, and not so much about the false positives, this does actually not improve our model at all, albeit a simple option to try. . Adjusting the RM to fraud detection . def get_model_results(X_train, y_train, X_test, y_test, model): model.fit(X_train, y_train) predicted = model.predict(X_test) print (classification_report(y_test, predicted)) print (confusion_matrix(y_test, predicted)) . # Change the model options model = RandomForestClassifier(bootstrap=True, ################################### class_weight={0:1, 1:12}, # 0: non-fraud , 1:fraud criterion=&#39;entropy&#39;, # Change depth of model max_depth=10, # Change the number of samples in leaf nodes min_samples_leaf=10, # Change the number of trees to use n_estimators=20, n_jobs=-1, random_state=5) # Run the function get_model_results get_model_results(X_train, y_train, X_test, y_test, model) . precision recall f1-score support 0.0 1.00 1.00 1.00 85296 1.0 0.91 0.82 0.86 147 micro avg 1.00 1.00 1.00 85443 macro avg 0.95 0.91 0.93 85443 weighted avg 1.00 1.00 1.00 85443 [[85284 12] [ 27 120]] . You can see by smartly defining more options in the model, you can obtain better predictions. You have effectively reduced the number of false negatives, i.e. you are catching more cases of fraud, whilst keeping the number of false positives low. In this exercise you&#39;ve manually changed the options of the model . GridSearchCV to find optimal parameters . from sklearn.model_selection import GridSearchCV . # Define the parameter sets to test param_grid = { &#39;n_estimators&#39;: [1, 30], &#39;max_features&#39;: [&#39;auto&#39;, &#39;log2&#39;], &#39;max_depth&#39;: [4, 8], &#39;criterion&#39;: [&#39;gini&#39;, &#39;entropy&#39;] } # Define the model to use model = RandomForestClassifier(random_state=5) ################################### # Combine the parameter sets with the defined model CV_model = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring=&#39;recall&#39;, n_jobs=-1) ################################### # Fit the model to our training data and obtain best parameters CV_model.fit(X_train, y_train) CV_model.best_params_ . {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 8, &#39;max_features&#39;: &#39;auto&#39;, &#39;n_estimators&#39;: 30} . Model results using GridSearchCV . # Input the optimal parameters in the model model = RandomForestClassifier(class_weight={0:1,1:12}, ################################### criterion=&#39;gini&#39;, n_estimators=30, max_features=&#39;log2&#39;, min_samples_leaf=10, max_depth=8, n_jobs=-1, random_state=5) # Get results from your model get_model_results(X_train, y_train, X_test, y_test, model) . precision recall f1-score support 0.0 1.00 1.00 1.00 85296 1.0 0.82 0.81 0.82 147 micro avg 1.00 1.00 1.00 85443 macro avg 0.91 0.90 0.91 85443 weighted avg 1.00 1.00 1.00 85443 [[85270 26] [ 28 119]] . We&#39;ve managed to improve your model even further. The number of false positives has now been slightly reduced even further, which means we are catching more cases of fraud. However, you see that the number of false negatives is still the same. That is that Precision-Recall trade-off in action. To decide which final model is best, you need to take into account how bad it is not to catch fraudsters, versus how many false positives the fraud analytics team can deal with. . Logistic Regression . # Define the Logistic Regression model with weights model = LogisticRegression(class_weight={0:1, 1:15}, random_state=5) ################################### # Get the model results get_model_results(X_train, y_train, X_test, y_test, model) . /Users/harshit/projects/oreilly/or_env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning. FutureWarning) . precision recall f1-score support 0.0 1.00 1.00 1.00 85296 1.0 0.78 0.82 0.80 147 micro avg 1.00 1.00 1.00 85443 macro avg 0.89 0.91 0.90 85443 weighted avg 1.00 1.00 1.00 85443 [[85262 34] [ 27 120]] . As you can see the Logistic Regression has quite different performance from the Random Forest. More false positives, but also a better Recall . Voting Classifier . from sklearn.tree import DecisionTreeClassifier . # Import the package from sklearn.ensemble import VotingClassifier ################################### # Define the three classifiers to use in the ensemble clf1 = LogisticRegression(class_weight={0:1, 1:15}, random_state=5) clf2 = RandomForestClassifier(class_weight={0:1, 1:12}, criterion=&#39;gini&#39;, max_depth=8, max_features=&#39;log2&#39;, min_samples_leaf=10, n_estimators=30, n_jobs=-1, random_state=5) clf3 = DecisionTreeClassifier(random_state=5, class_weight=&quot;balanced&quot;) # Combine the classifiers in the ensemble model ensemble_model = VotingClassifier(estimators=[(&#39;lr&#39;, clf1), (&#39;rf&#39;, clf2), (&#39;dt&#39;, clf3)], voting=&#39;hard&#39;) # Get the results get_model_results(X_train, y_train, X_test, y_test, ensemble_model) . /Users/harshit/projects/oreilly/or_env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning. FutureWarning) . precision recall f1-score support 0.0 1.00 1.00 1.00 85296 1.0 0.85 0.82 0.83 147 micro avg 1.00 1.00 1.00 85443 macro avg 0.93 0.91 0.92 85443 weighted avg 1.00 1.00 1.00 85443 [[85275 21] [ 27 120]] . We can see that by combining the classifiers, you can take the best of multiple models. You&#39;ve increased the cases of fraud you are catching from 119 to 120, and you only have 5 less false positives. If you do care about catching as many fraud cases as you can, whilst keeping the false positives low, this is a pretty good trade-off. The Logistic Regression as a standalone was quite bad in terms of false positives, and the Random Forest was worse in terms of false negatives. By combining these together you indeed managed to improve performance. . Adjust weights within the Voting Classifier . # Define the ensemble model ensemble_model = VotingClassifier( ################################### estimators=[(&#39;lr&#39;, clf1), (&#39;rf&#39;, clf2), (&#39;gnb&#39;, clf3)], voting=&#39;soft&#39;, weights=[1, 4, 1], flatten_transform=True) # Get results get_model_results(X_train, y_train, X_test, y_test, ensemble_model) . /Users/harshit/projects/oreilly/or_env/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning. FutureWarning) . precision recall f1-score support 0.0 1.00 1.00 1.00 85296 1.0 0.85 0.82 0.83 147 micro avg 1.00 1.00 1.00 85443 macro avg 0.92 0.91 0.92 85443 weighted avg 1.00 1.00 1.00 85443 [[85274 22] [ 27 120]] . ensemble_model.estimators_ . [LogisticRegression(C=1.0, class_weight={0: 1, 1: 15}, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class=&#39;warn&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=5, solver=&#39;warn&#39;, tol=0.0001, verbose=0, warm_start=False), RandomForestClassifier(bootstrap=True, class_weight={0: 1, 1: 12}, criterion=&#39;gini&#39;, max_depth=8, max_features=&#39;log2&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=10, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=-1, oob_score=False, random_state=5, verbose=0, warm_start=False), DecisionTreeClassifier(class_weight=&#39;balanced&#39;, criterion=&#39;gini&#39;, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=5, splitter=&#39;best&#39;)] .",
            "url": "https://ishmaelasabere.github.io/Data-Science/2020/08/25/Fraud-Analytics.html",
            "relUrl": "/2020/08/25/Fraud-Analytics.html",
            "date": " • Aug 25, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ishmaelasabere.github.io/Data-Science/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ishmaelasabere.github.io/Data-Science/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ishmaelasabere.github.io/Data-Science/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ishmaelasabere.github.io/Data-Science/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}